{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task:** Classification  \n",
    "**data:** MNIST -- handwritten digits between 0 and 9, in grayscale  \n",
    "**Loss function:** Cross Entropy Loss (more on this later)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture:** We use a multilayer perceptron\n",
    "\n",
    "Each layer consists of what we can think of neurons or nodes. Each node takes the outputs of all the neurons in the previous layer as input, in the form of a vector. It then performs a linear transformation on the input (wx+b) and then puts it through a non-linear transformation. The final output is a scalar-value, but since the layer (generally speaking) has multiple nodes, the output of the layer itself is a vector.\n",
    "\n",
    "So if z is the output\n",
    "\n",
    "$$z = \\sigma(\\vec{w}^T\\vec{x} + \\vec{b}) = \\sigma (\\sum w_i \\cdot x_i + b_i)$$\n",
    "\n",
    "where $\\sigma$ is a non-linear function. In our case we use \"ReLU\", where $$ReLU(x) = max(0, x)$$ In otherwords, it just zeros-out negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP,self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128) #Input layer \n",
    "        self.fc2 = nn.Linear(128, 64) #Hidden layer 1\n",
    "        self.fc3 = nn.Linear(64, 32) #Hidden layer 2\n",
    "        self.fc4 = nn.Linear(32, 10) #Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = f.relu(self.fc1(x))\n",
    "        x = f.relu(self.fc2(x))\n",
    "        x = f.relu(self.fc3(x))\n",
    "        x = f.relu(self.fc4(x))\n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have defined a multilayer perceptron with 2 hidden layers. The input layer takes in input tensors of size 784 because that is the size of our data: MNIST data consists of 28x28=784 pixel images.\n",
    "\n",
    "After that we have 2 hidden layers because, why not?\n",
    "\n",
    "The output layer outputs a 10-dimensional vector because there are 10 possible outputs to the data: 0,1,2,3,4,5,6,7,8,9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get back to the loss function we mentioned earlier. We use \"Cross Entropy Loss\", given by\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\cdot \\log{\\hat{y}_{i,c}}$$\n",
    "\n",
    "Where N is the number of inputs, and c is the number of classes.\n",
    "\n",
    "$y_{i,c}$ is 1 if sample ***i*** belongs to class ***c*** and 0 otherwise.\n",
    "\n",
    "$\\hat{y}_{i,c}$ is the predicted probability that sample ***i*** belongs to class ***c***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyperparameters***\n",
    "\n",
    "Our main hyperparameter is the learning rate, which scales the gradient.\n",
    "\n",
    "Later on, we will set the number of training epochs. That number is also a hyperparameter.\n",
    "\n",
    "In the next step where we preprocess the data, our batch sizes are also hyperparameters. The batch size is how many examples/datapoints we go through before updating the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()                          # suitable for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) #lr is the 'learning rate', which is a hyperparameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**: We are normalizing and flattening the data so everything is between 0 and 1.  \n",
    "\n",
    "*What does that mean, you ask?*\n",
    "\n",
    "--> Flattening: we are classifying images, which are 2-dimensional. We need input that is 1-dimensional so we can put it into the network, so 'flattening' just means converting it to a 1D array\n",
    "\n",
    "--> Normalizing: Our images are grayscale values between 0 and 255. Normalizing means we smoosh that to values between 0 and 1, where 255 corresponds to 1. This prevents large values from being overly influential, or compounding into a big number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**\n",
    "\n",
    "Our training algorithm is backpropogation with gradient descent.\n",
    "\n",
    "After making a forward pass through the network, we make a backward pass (backpropogation) where we compute the loss of each node and update it using the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6031\n",
      "Epoch 2/10, Loss: 0.3798\n",
      "Epoch 3/10, Loss: 0.3356\n",
      "Epoch 4/10, Loss: 0.3110\n",
      "Epoch 5/10, Loss: 0.2933\n",
      "Epoch 6/10, Loss: 0.2816\n",
      "Epoch 7/10, Loss: 0.2737\n",
      "Epoch 8/10, Loss: 0.2659\n",
      "Epoch 9/10, Loss: 0.2615\n",
      "Epoch 10/10, Loss: 0.2547\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # training mode (e.g., enables dropout if used)\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.51%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting**: The model becomes to specialized on the training data, so that it doesn't generalize well. This can be caused by using too many training epochs and to strict of a loss function.\n",
    "\n",
    "--> You can tell a model is overfitting when it performs extremely well on training/test data but performs poorly on validation data.\n",
    "\n",
    "To fix it, use fewer training epochs, or less complicated models (complicated models can become exteremly specialized very easily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Accuracy, Precision, and Recall:***\n",
    "\n",
    "**Notation**: TP = True Positive, FP = False Positive, TN = True Negative, FN = False Negative\n",
    "\n",
    "**Accuracy** is the percentage of predictions the model got right. So its $\\frac{TP+TN}{TP+FP+TN+FN}$\n",
    "\n",
    "**Precision** is the number of 'true' predictions that were actually correct, given as $\\frac{TP}{TP+FP}$\n",
    "\n",
    "**Recall** is the number of 'positive' examples the model actually identified, given as $\\frac{TP}{TP+FN}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
